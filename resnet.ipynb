{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_model_summary as pms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Test Train Split 1\n",
      "Test Train Split 2\n",
      "Loading Leads\n",
      "Dataset shape:  (25860, 12)\n",
      "Dataset shape:  (8620, 12)\n",
      "Dataset shape:  (8621, 12)\n",
      "Delete Missing Rows\n"
     ]
    }
   ],
   "source": [
    "def dataloader(url):\n",
    "    print(\"Loading Data\")\n",
    "    dataset = pd.read_pickle(url)\n",
    "    labels = dataset.iloc[:, 16:]\n",
    "    dataset = dataset.iloc[:, 4:16]\n",
    "\n",
    "    _, weightindex = weight_metric()\n",
    "    labels = labels[weightindex].to_numpy() #[:27]\n",
    "    del weightindex\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    print(\"Test Train Split 1\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "    del dataset\n",
    "    del labels\n",
    "\n",
    "\n",
    "    # train-validation-test split = 60/20/20\n",
    "    print(\"Test Train Split 2\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    print(\"Loading Leads\")\n",
    "    X_train = lead_loader(X_train)\n",
    "    X_val = lead_loader(X_val)\n",
    "    X_test = lead_loader(X_test)\n",
    "\n",
    "    print(\"Delete Missing Rows\")\n",
    "    X_train, y_train = delete_missing_rows(X_train, y_train)\n",
    "    X_val, y_val = delete_missing_rows(X_val, y_val)\n",
    "    X_test, y_test = delete_missing_rows(X_test, y_test)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_val, y_val\n",
    "\n",
    "# Convert ECG leads from Dataframe to NumPy array (used to convert to tensor later)\n",
    "def lead_loader(dataset):\n",
    "\n",
    "    dataset = dataset.to_numpy()\n",
    "    print(\"Dataset shape: \", dataset.shape)\n",
    "    testingleads = np.zeros([dataset.shape[0], dataset.shape[1], 5000])\n",
    "    \n",
    "    for i in range(0,12):\n",
    "        for j in range(0, dataset.shape[0]):\n",
    "            testingleads[j, i, :] = dataset[j, i]\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "\n",
    "    return testingleads\n",
    "\n",
    "# Loading weight metric from challenge to use the 27 classes\n",
    "def weight_metric():\n",
    "    weights = pd.read_csv(\"../Data/weights.csv\", index_col=0)\n",
    "    ctcodes = pd.read_csv(\"../Data/Dx_map.csv\")\n",
    "    ctcodes = ctcodes.iloc[:, 1:]\n",
    "    replacedict = dict(zip(ctcodes.iloc[:,0], ctcodes.iloc[:,1]))\n",
    "\n",
    "    weights.columns = weights.columns.astype(int)\n",
    "    weights.rename(columns=replacedict, index=replacedict, inplace=True)\n",
    "    weightindex = np.array(weights.index)\n",
    "    return weights, weightindex\n",
    "\n",
    "\n",
    "# Delete rows where labels are missing\n",
    "def delete_missing_rows(leads, labels):\n",
    "    print(\"Deleting Rows/n\")\n",
    "    indexvals=[]\n",
    "    for i in range(0, labels.shape[0]):\n",
    "        if len(np.unique(labels[i,:], axis=0)) == 2:\n",
    "            indexvals.append(i)\n",
    "    leads = np.take(leads, indexvals, axis=0)\n",
    "    labels = np.take(labels, indexvals, axis=0)\n",
    "\n",
    "    del indexvals\n",
    "    gc.collect()\n",
    "\n",
    "    return leads, labels\n",
    "\n",
    "\n",
    "\n",
    "trainingleads, testingleads, traininglabels, testinglabels, validationleads, validationlabels = dataloader(\"../Data/fullecgdata.pkl\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL TRAIN AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EPOCH 0\n",
      "02:06:28 , Epoch: 0 [64/22693], Loss: 0.716201428336262 Accuracy: 0.0\n",
      "02:06:28 , Epoch: 0 [128/22693], Loss: 0.622020919574266 Accuracy: 0.0\n",
      "02:06:29 , Epoch: 0 [192/22693], Loss: 0.5413420607581675 Accuracy: 0.0\n",
      "02:06:30 , Epoch: 0 [256/22693], Loss: 0.48291406987081426 Accuracy: 0.0\n",
      "02:06:31 , Epoch: 0 [320/22693], Loss: 0.4367429940066323 Accuracy: 0.0\n",
      "02:06:32 , Epoch: 0 [384/22693], Loss: 0.39930014745514963 Accuracy: 0.03125\n",
      "02:06:33 , Epoch: 0 [448/22693], Loss: 0.37218459750709415 Accuracy: 0.046875\n",
      "02:06:33 , Epoch: 0 [512/22693], Loss: 0.3502240644374052 Accuracy: 0.09375\n",
      "02:06:34 , Epoch: 0 [576/22693], Loss: 0.3330145130146082 Accuracy: 0.21875\n",
      "02:06:35 , Epoch: 0 [640/22693], Loss: 0.3188814173284591 Accuracy: 0.296875\n",
      "02:06:35 , Epoch: 0 [704/22693], Loss: 0.3090243739397188 Accuracy: 0.21875\n",
      "02:06:36 , Epoch: 0 [768/22693], Loss: 0.29953946939812576 Accuracy: 0.265625\n",
      "02:06:37 , Epoch: 0 [832/22693], Loss: 0.29153851731940783 Accuracy: 0.34375\n",
      "02:06:38 , Epoch: 0 [896/22693], Loss: 0.28297447154980937 Accuracy: 0.359375\n",
      "02:06:38 , Epoch: 0 [960/22693], Loss: 0.27694655770474846 Accuracy: 0.328125\n",
      "02:06:39 , Epoch: 0 [1024/22693], Loss: 0.2711129284154604 Accuracy: 0.359375\n",
      "02:06:40 , Epoch: 0 [1088/22693], Loss: 0.267775821769963 Accuracy: 0.265625\n",
      "02:06:41 , Epoch: 0 [1152/22693], Loss: 0.2631592676757766 Accuracy: 0.359375\n",
      "02:06:42 , Epoch: 0 [1216/22693], Loss: 0.25977050651635486 Accuracy: 0.265625\n",
      "02:06:42 , Epoch: 0 [1280/22693], Loss: 0.25596479567434793 Accuracy: 0.25\n",
      "02:06:43 , Epoch: 0 [1344/22693], Loss: 0.25294055569066787 Accuracy: 0.203125\n",
      "02:06:44 , Epoch: 0 [1408/22693], Loss: 0.24967561269423022 Accuracy: 0.28125\n",
      "02:06:44 , Epoch: 0 [1472/22693], Loss: 0.246647055722388 Accuracy: 0.328125\n",
      "02:06:45 , Epoch: 0 [1536/22693], Loss: 0.24472649216852135 Accuracy: 0.15625\n",
      "02:06:46 , Epoch: 0 [1600/22693], Loss: 0.2424099429552559 Accuracy: 0.171875\n",
      "02:06:47 , Epoch: 0 [1664/22693], Loss: 0.24009683902048137 Accuracy: 0.296875\n",
      "02:06:48 , Epoch: 0 [1728/22693], Loss: 0.2390016507518917 Accuracy: 0.171875\n",
      "02:06:48 , Epoch: 0 [1792/22693], Loss: 0.23672599922215287 Accuracy: 0.296875\n",
      "02:06:49 , Epoch: 0 [1856/22693], Loss: 0.23458841079282994 Accuracy: 0.28125\n",
      "02:06:50 , Epoch: 0 [1920/22693], Loss: 0.23267432511709316 Accuracy: 0.234375\n",
      "02:06:50 , Epoch: 0 [1984/22693], Loss: 0.23081680663053955 Accuracy: 0.234375\n",
      "02:06:51 , Epoch: 0 [2048/22693], Loss: 0.22962127244577812 Accuracy: 0.15625\n",
      "02:06:52 , Epoch: 0 [2112/22693], Loss: 0.22795972420649865 Accuracy: 0.234375\n",
      "02:06:53 , Epoch: 0 [2176/22693], Loss: 0.22671036543576278 Accuracy: 0.25\n",
      "02:06:54 , Epoch: 0 [2240/22693], Loss: 0.22530637262329808 Accuracy: 0.171875\n",
      "02:06:54 , Epoch: 0 [2304/22693], Loss: 0.22355847392236136 Accuracy: 0.34375\n",
      "02:06:55 , Epoch: 0 [2368/22693], Loss: 0.2219983404126927 Accuracy: 0.328125\n",
      "02:06:56 , Epoch: 0 [2432/22693], Loss: 0.22048367199008548 Accuracy: 0.390625\n",
      "02:06:57 , Epoch: 0 [2496/22693], Loss: 0.2189712446200359 Accuracy: 0.359375\n",
      "02:06:57 , Epoch: 0 [2560/22693], Loss: 0.21856089159167502 Accuracy: 0.171875\n",
      "02:06:58 , Epoch: 0 [2624/22693], Loss: 0.21753909849919015 Accuracy: 0.375\n",
      "02:06:59 , Epoch: 0 [2688/22693], Loss: 0.21634962674711122 Accuracy: 0.28125\n",
      "02:07:00 , Epoch: 0 [2752/22693], Loss: 0.21555375733091797 Accuracy: 0.296875\n",
      "02:07:00 , Epoch: 0 [2816/22693], Loss: 0.21466575087670287 Accuracy: 0.25\n",
      "02:07:01 , Epoch: 0 [2880/22693], Loss: 0.21372764567746622 Accuracy: 0.265625\n",
      "02:07:02 , Epoch: 0 [2944/22693], Loss: 0.21269375413988637 Accuracy: 0.328125\n",
      "02:07:03 , Epoch: 0 [3008/22693], Loss: 0.21226880364520959 Accuracy: 0.296875\n",
      "02:07:03 , Epoch: 0 [3072/22693], Loss: 0.21156969362718606 Accuracy: 0.25\n",
      "02:07:04 , Epoch: 0 [3136/22693], Loss: 0.2106644603396854 Accuracy: 0.328125\n",
      "02:07:05 , Epoch: 0 [3200/22693], Loss: 0.21027178344292996 Accuracy: 0.28125\n",
      "02:07:06 , Epoch: 0 [3264/22693], Loss: 0.20963928555177172 Accuracy: 0.1875\n",
      "02:07:06 , Epoch: 0 [3328/22693], Loss: 0.20887792233036195 Accuracy: 0.296875\n",
      "02:07:07 , Epoch: 0 [3392/22693], Loss: 0.20890535303656063 Accuracy: 0.1875\n",
      "02:07:09 , Epoch: 0 [3456/22693], Loss: 0.20815894217479347 Accuracy: 0.28125\n",
      "02:07:09 , Epoch: 0 [3520/22693], Loss: 0.20755825920062762 Accuracy: 0.28125\n",
      "02:07:10 , Epoch: 0 [3584/22693], Loss: 0.2069409493478854 Accuracy: 0.265625\n",
      "02:07:11 , Epoch: 0 [3648/22693], Loss: 0.20646657906339871 Accuracy: 0.25\n",
      "02:07:11 , Epoch: 0 [3712/22693], Loss: 0.20596838368975443 Accuracy: 0.25\n",
      "02:07:12 , Epoch: 0 [3776/22693], Loss: 0.20541659110819097 Accuracy: 0.265625\n",
      "02:07:13 , Epoch: 0 [3840/22693], Loss: 0.20511939848473182 Accuracy: 0.25\n",
      "02:07:13 , Epoch: 0 [3904/22693], Loss: 0.2052199503440208 Accuracy: 0.25\n",
      "02:07:14 , Epoch: 0 [3968/22693], Loss: 0.20490127465808952 Accuracy: 0.203125\n",
      "02:07:15 , Epoch: 0 [4032/22693], Loss: 0.20438946367802877 Accuracy: 0.3125\n",
      "02:07:16 , Epoch: 0 [4096/22693], Loss: 0.20411456460587296 Accuracy: 0.171875\n",
      "02:07:17 , Epoch: 0 [4160/22693], Loss: 0.20346579591328381 Accuracy: 0.296875\n",
      "02:07:17 , Epoch: 0 [4224/22693], Loss: 0.20323867362183923 Accuracy: 0.21875\n",
      "02:07:18 , Epoch: 0 [4288/22693], Loss: 0.2028979039586648 Accuracy: 0.203125\n",
      "02:07:19 , Epoch: 0 [4352/22693], Loss: 0.20241147472215687 Accuracy: 0.265625\n",
      "02:07:20 , Epoch: 0 [4416/22693], Loss: 0.2023721797241813 Accuracy: 0.1875\n",
      "02:07:20 , Epoch: 0 [4480/22693], Loss: 0.20208361970414948 Accuracy: 0.328125\n",
      "02:07:21 , Epoch: 0 [4544/22693], Loss: 0.20166686649744886 Accuracy: 0.328125\n",
      "02:07:22 , Epoch: 0 [4608/22693], Loss: 0.20129011598582813 Accuracy: 0.25\n",
      "02:07:23 , Epoch: 0 [4672/22693], Loss: 0.20102627551592978 Accuracy: 0.3125\n",
      "02:07:24 , Epoch: 0 [4736/22693], Loss: 0.20060931287694495 Accuracy: 0.203125\n",
      "02:07:24 , Epoch: 0 [4800/22693], Loss: 0.20011316934056927 Accuracy: 0.359375\n",
      "02:07:25 , Epoch: 0 [4864/22693], Loss: 0.19986186331472927 Accuracy: 0.265625\n",
      "02:07:26 , Epoch: 0 [4928/22693], Loss: 0.19980072287221728 Accuracy: 0.25\n",
      "02:07:28 , Epoch: 0 [4992/22693], Loss: 0.19946110986159946 Accuracy: 0.25\n",
      "02:07:29 , Epoch: 0 [5056/22693], Loss: 0.19917606318657866 Accuracy: 0.328125\n",
      "02:07:29 , Epoch: 0 [5120/22693], Loss: 0.19891645975670622 Accuracy: 0.234375\n",
      "02:07:30 , Epoch: 0 [5184/22693], Loss: 0.198509098722426 Accuracy: 0.375\n",
      "02:07:31 , Epoch: 0 [5248/22693], Loss: 0.19822731261452622 Accuracy: 0.265625\n",
      "02:07:32 , Epoch: 0 [5312/22693], Loss: 0.19835171999248394 Accuracy: 0.15625\n",
      "02:07:32 , Epoch: 0 [5376/22693], Loss: 0.19804613443967034 Accuracy: 0.328125\n",
      "02:07:33 , Epoch: 0 [5440/22693], Loss: 0.19781746897457483 Accuracy: 0.296875\n",
      "02:07:34 , Epoch: 0 [5504/22693], Loss: 0.19743259936219415 Accuracy: 0.34375\n",
      "02:07:35 , Epoch: 0 [5568/22693], Loss: 0.19693391910287436 Accuracy: 0.390625\n",
      "02:07:36 , Epoch: 0 [5632/22693], Loss: 0.1971643887973484 Accuracy: 0.234375\n",
      "02:07:37 , Epoch: 0 [5696/22693], Loss: 0.19690261555004265 Accuracy: 0.3125\n",
      "02:07:38 , Epoch: 0 [5760/22693], Loss: 0.19664491502798834 Accuracy: 0.3125\n",
      "02:07:39 , Epoch: 0 [5824/22693], Loss: 0.19648502730623255 Accuracy: 0.34375\n",
      "02:07:39 , Epoch: 0 [5888/22693], Loss: 0.19628651856134335 Accuracy: 0.3125\n",
      "02:07:40 , Epoch: 0 [5952/22693], Loss: 0.19596783450616762 Accuracy: 0.28125\n",
      "02:07:41 , Epoch: 0 [6016/22693], Loss: 0.19613119146770547 Accuracy: 0.265625\n",
      "02:07:42 , Epoch: 0 [6080/22693], Loss: 0.1961862083525654 Accuracy: 0.203125\n",
      "02:07:42 , Epoch: 0 [6144/22693], Loss: 0.19624607538513472 Accuracy: 0.203125\n",
      "02:07:43 , Epoch: 0 [6208/22693], Loss: 0.1962484767572831 Accuracy: 0.265625\n",
      "02:07:44 , Epoch: 0 [6272/22693], Loss: 0.19627249751846304 Accuracy: 0.109375\n",
      "02:07:45 , Epoch: 0 [6336/22693], Loss: 0.19604791391786847 Accuracy: 0.125\n",
      "02:07:45 , Epoch: 0 [6400/22693], Loss: 0.19603815844481157 Accuracy: 0.15625\n",
      "02:07:46 , Epoch: 0 [6464/22693], Loss: 0.19575784516088518 Accuracy: 0.203125\n",
      "02:07:47 , Epoch: 0 [6528/22693], Loss: 0.1955570017946389 Accuracy: 0.078125\n",
      "02:07:48 , Epoch: 0 [6592/22693], Loss: 0.19530315712016905 Accuracy: 0.15625\n",
      "02:07:49 , Epoch: 0 [6656/22693], Loss: 0.19526776293514678 Accuracy: 0.21875\n",
      "02:07:50 , Epoch: 0 [6720/22693], Loss: 0.19517000216748528 Accuracy: 0.234375\n",
      "02:07:51 , Epoch: 0 [6784/22693], Loss: 0.19527458757579275 Accuracy: 0.203125\n",
      "02:07:51 , Epoch: 0 [6848/22693], Loss: 0.19508992861923588 Accuracy: 0.3125\n",
      "02:07:52 , Epoch: 0 [6912/22693], Loss: 0.19501844500022225 Accuracy: 0.234375\n",
      "02:07:53 , Epoch: 0 [6976/22693], Loss: 0.19471534404454646 Accuracy: 0.375\n",
      "02:07:54 , Epoch: 0 [7040/22693], Loss: 0.1944282734955805 Accuracy: 0.34375\n",
      "02:07:54 , Epoch: 0 [7104/22693], Loss: 0.19433772745428923 Accuracy: 0.21875\n",
      "02:07:55 , Epoch: 0 [7168/22693], Loss: 0.19416835375548328 Accuracy: 0.34375\n",
      "02:07:56 , Epoch: 0 [7232/22693], Loss: 0.1941669373956931 Accuracy: 0.21875\n",
      "02:07:57 , Epoch: 0 [7296/22693], Loss: 0.1940288474491252 Accuracy: 0.328125\n",
      "02:07:58 , Epoch: 0 [7360/22693], Loss: 0.1938316753668005 Accuracy: 0.28125\n",
      "02:07:59 , Epoch: 0 [7424/22693], Loss: 0.1935614701268232 Accuracy: 0.265625\n",
      "02:07:59 , Epoch: 0 [7488/22693], Loss: 0.19337085222385786 Accuracy: 0.375\n",
      "02:08:00 , Epoch: 0 [7552/22693], Loss: 0.19339093086536557 Accuracy: 0.28125\n",
      "02:08:01 , Epoch: 0 [7616/22693], Loss: 0.19325691231476763 Accuracy: 0.25\n",
      "02:08:02 , Epoch: 0 [7680/22693], Loss: 0.19296112481959604 Accuracy: 0.390625\n",
      "02:08:03 , Epoch: 0 [7744/22693], Loss: 0.19281575087944647 Accuracy: 0.09375\n",
      "02:08:04 , Epoch: 0 [7808/22693], Loss: 0.19275942254132497 Accuracy: 0.265625\n",
      "02:08:05 , Epoch: 0 [7872/22693], Loss: 0.19269128382868012 Accuracy: 0.1875\n",
      "02:08:06 , Epoch: 0 [7936/22693], Loss: 0.19262691707957483 Accuracy: 0.28125\n",
      "02:08:07 , Epoch: 0 [8000/22693], Loss: 0.19249258345036468 Accuracy: 0.359375\n",
      "02:08:08 , Epoch: 0 [8064/22693], Loss: 0.19235775813149106 Accuracy: 0.359375\n",
      "02:08:09 , Epoch: 0 [8128/22693], Loss: 0.19219719156568796 Accuracy: 0.3125\n",
      "02:08:10 , Epoch: 0 [8192/22693], Loss: 0.19182694617455912 Accuracy: 0.390625\n",
      "02:08:11 , Epoch: 0 [8256/22693], Loss: 0.19179582411209095 Accuracy: 0.25\n",
      "02:08:12 , Epoch: 0 [8320/22693], Loss: 0.1915790914213084 Accuracy: 0.28125\n",
      "02:08:13 , Epoch: 0 [8384/22693], Loss: 0.19152406749760434 Accuracy: 0.3125\n",
      "02:08:14 , Epoch: 0 [8448/22693], Loss: 0.19149932079899248 Accuracy: 0.25\n",
      "02:08:15 , Epoch: 0 [8512/22693], Loss: 0.19126918609611868 Accuracy: 0.34375\n",
      "02:08:16 , Epoch: 0 [8576/22693], Loss: 0.19126507867929415 Accuracy: 0.234375\n",
      "02:08:17 , Epoch: 0 [8640/22693], Loss: 0.19126428320486236 Accuracy: 0.34375\n",
      "02:08:18 , Epoch: 0 [8704/22693], Loss: 0.19121049559043754 Accuracy: 0.34375\n",
      "02:08:20 , Epoch: 0 [8768/22693], Loss: 0.19115368621009513 Accuracy: 0.28125\n",
      "02:08:21 , Epoch: 0 [8832/22693], Loss: 0.1909855868115642 Accuracy: 0.265625\n",
      "02:08:22 , Epoch: 0 [8896/22693], Loss: 0.1908895117371418 Accuracy: 0.203125\n",
      "02:08:24 , Epoch: 0 [8960/22693], Loss: 0.19079832578366274 Accuracy: 0.265625\n",
      "02:08:26 , Epoch: 0 [9024/22693], Loss: 0.19059704169712388 Accuracy: 0.390625\n",
      "02:08:27 , Epoch: 0 [9088/22693], Loss: 0.19058453110964485 Accuracy: 0.1875\n",
      "02:08:29 , Epoch: 0 [9152/22693], Loss: 0.19046752836396064 Accuracy: 0.203125\n",
      "02:08:31 , Epoch: 0 [9216/22693], Loss: 0.19048376025453823 Accuracy: 0.203125\n",
      "02:08:33 , Epoch: 0 [9280/22693], Loss: 0.1904951571687379 Accuracy: 0.21875\n",
      "02:08:34 , Epoch: 0 [9344/22693], Loss: 0.19050037959495505 Accuracy: 0.265625\n",
      "02:08:35 , Epoch: 0 [9408/22693], Loss: 0.1905976365393995 Accuracy: 0.21875\n",
      "02:08:36 , Epoch: 0 [9472/22693], Loss: 0.19025472166878396 Accuracy: 0.421875\n",
      "02:08:37 , Epoch: 0 [9536/22693], Loss: 0.190177121962819 Accuracy: 0.25\n",
      "02:08:37 , Epoch: 0 [9600/22693], Loss: 0.19013973923046193 Accuracy: 0.265625\n",
      "02:08:38 , Epoch: 0 [9664/22693], Loss: 0.18997829602086458 Accuracy: 0.375\n",
      "02:08:39 , Epoch: 0 [9728/22693], Loss: 0.19002352861535732 Accuracy: 0.25\n",
      "02:08:39 , Epoch: 0 [9792/22693], Loss: 0.1899129884795784 Accuracy: 0.25\n",
      "02:08:40 , Epoch: 0 [9856/22693], Loss: 0.18968065770096493 Accuracy: 0.375\n",
      "02:08:41 , Epoch: 0 [9920/22693], Loss: 0.18955010467285727 Accuracy: 0.359375\n",
      "02:08:42 , Epoch: 0 [9984/22693], Loss: 0.18959436789055284 Accuracy: 0.296875\n",
      "02:08:42 , Epoch: 0 [10048/22693], Loss: 0.18939399980956043 Accuracy: 0.4375\n",
      "02:08:43 , Epoch: 0 [10112/22693], Loss: 0.18922990006924967 Accuracy: 0.375\n",
      "02:08:44 , Epoch: 0 [10176/22693], Loss: 0.18922867688850026 Accuracy: 0.28125\n",
      "02:08:44 , Epoch: 0 [10240/22693], Loss: 0.18913959656362586 Accuracy: 0.3125\n",
      "02:08:45 , Epoch: 0 [10304/22693], Loss: 0.18914190602444056 Accuracy: 0.265625\n",
      "02:08:46 , Epoch: 0 [10368/22693], Loss: 0.18903273800612633 Accuracy: 0.375\n",
      "02:08:46 , Epoch: 0 [10432/22693], Loss: 0.18885266142590845 Accuracy: 0.34375\n",
      "02:08:47 , Epoch: 0 [10496/22693], Loss: 0.1887924693133093 Accuracy: 0.25\n",
      "02:08:48 , Epoch: 0 [10560/22693], Loss: 0.1886333207727758 Accuracy: 0.296875\n",
      "02:08:48 , Epoch: 0 [10624/22693], Loss: 0.18850318396768112 Accuracy: 0.328125\n",
      "02:08:49 , Epoch: 0 [10688/22693], Loss: 0.18845636397607465 Accuracy: 0.28125\n",
      "02:08:50 , Epoch: 0 [10752/22693], Loss: 0.18844881792875562 Accuracy: 0.234375\n",
      "02:08:50 , Epoch: 0 [10816/22693], Loss: 0.18840896471386337 Accuracy: 0.375\n",
      "02:08:51 , Epoch: 0 [10880/22693], Loss: 0.18842059231680433 Accuracy: 0.25\n",
      "02:08:52 , Epoch: 0 [10944/22693], Loss: 0.18827300577819248 Accuracy: 0.265625\n",
      "02:08:52 , Epoch: 0 [11008/22693], Loss: 0.1883374648976307 Accuracy: 0.25\n",
      "02:08:53 , Epoch: 0 [11072/22693], Loss: 0.18820431690228995 Accuracy: 0.3125\n",
      "02:08:54 , Epoch: 0 [11136/22693], Loss: 0.18817871038559425 Accuracy: 0.296875\n",
      "02:08:55 , Epoch: 0 [11200/22693], Loss: 0.18797185628771684 Accuracy: 0.40625\n",
      "02:08:55 , Epoch: 0 [11264/22693], Loss: 0.18810573087946122 Accuracy: 0.203125\n",
      "02:08:56 , Epoch: 0 [11328/22693], Loss: 0.18785244829898393 Accuracy: 0.390625\n",
      "02:08:57 , Epoch: 0 [11392/22693], Loss: 0.18761015320035904 Accuracy: 0.40625\n",
      "02:08:57 , Epoch: 0 [11456/22693], Loss: 0.18746300758965892 Accuracy: 0.328125\n",
      "02:08:58 , Epoch: 0 [11520/22693], Loss: 0.18741633907101848 Accuracy: 0.28125\n",
      "02:08:59 , Epoch: 0 [11584/22693], Loss: 0.18712153435838325 Accuracy: 0.5\n",
      "02:09:00 , Epoch: 0 [11648/22693], Loss: 0.18700064425243565 Accuracy: 0.421875\n",
      "02:09:00 , Epoch: 0 [11712/22693], Loss: 0.18681328767936214 Accuracy: 0.390625\n",
      "02:09:01 , Epoch: 0 [11776/22693], Loss: 0.18672713635668023 Accuracy: 0.328125\n",
      "02:09:02 , Epoch: 0 [11840/22693], Loss: 0.18669525935326015 Accuracy: 0.3125\n",
      "02:09:02 , Epoch: 0 [11904/22693], Loss: 0.18654414336052608 Accuracy: 0.375\n",
      "02:09:03 , Epoch: 0 [11968/22693], Loss: 0.1865686930304672 Accuracy: 0.25\n",
      "02:09:04 , Epoch: 0 [12032/22693], Loss: 0.18648746284454287 Accuracy: 0.28125\n",
      "02:09:04 , Epoch: 0 [12096/22693], Loss: 0.186256291416999 Accuracy: 0.390625\n",
      "02:09:05 , Epoch: 0 [12160/22693], Loss: 0.18622888539929577 Accuracy: 0.265625\n",
      "02:09:06 , Epoch: 0 [12224/22693], Loss: 0.18617244990610793 Accuracy: 0.25\n",
      "02:09:06 , Epoch: 0 [12288/22693], Loss: 0.18613256621074628 Accuracy: 0.28125\n",
      "02:09:07 , Epoch: 0 [12352/22693], Loss: 0.18612644374332338 Accuracy: 0.234375\n",
      "02:09:08 , Epoch: 0 [12416/22693], Loss: 0.18603043865816263 Accuracy: 0.28125\n",
      "02:09:08 , Epoch: 0 [12480/22693], Loss: 0.18596979001105993 Accuracy: 0.28125\n",
      "02:09:09 , Epoch: 0 [12544/22693], Loss: 0.18585079381591116 Accuracy: 0.390625\n",
      "02:09:10 , Epoch: 0 [12608/22693], Loss: 0.1857025585120546 Accuracy: 0.28125\n",
      "02:09:10 , Epoch: 0 [12672/22693], Loss: 0.18572296107420233 Accuracy: 0.234375\n",
      "02:09:12 , Epoch: 0 [12736/22693], Loss: 0.18572261627873454 Accuracy: 0.21875\n",
      "02:09:13 , Epoch: 0 [12800/22693], Loss: 0.18563033528228273 Accuracy: 0.3125\n",
      "02:09:13 , Epoch: 0 [12864/22693], Loss: 0.18566254575984079 Accuracy: 0.265625\n",
      "02:09:14 , Epoch: 0 [12928/22693], Loss: 0.18554022986079124 Accuracy: 0.3125\n",
      "02:09:15 , Epoch: 0 [12992/22693], Loss: 0.1855356272928273 Accuracy: 0.25\n",
      "02:09:15 , Epoch: 0 [13056/22693], Loss: 0.18549960962304604 Accuracy: 0.265625\n",
      "02:09:16 , Epoch: 0 [13120/22693], Loss: 0.1854015507227503 Accuracy: 0.265625\n",
      "02:09:17 , Epoch: 0 [13184/22693], Loss: 0.1853845660309322 Accuracy: 0.265625\n",
      "02:09:17 , Epoch: 0 [13248/22693], Loss: 0.18545963888910383 Accuracy: 0.046875\n",
      "02:09:18 , Epoch: 0 [13312/22693], Loss: 0.1853115210644617 Accuracy: 0.328125\n",
      "02:09:19 , Epoch: 0 [13376/22693], Loss: 0.18524051551375523 Accuracy: 0.265625\n",
      "02:09:19 , Epoch: 0 [13440/22693], Loss: 0.18523658827007272 Accuracy: 0.296875\n",
      "02:09:20 , Epoch: 0 [13504/22693], Loss: 0.18518345169589726 Accuracy: 0.296875\n",
      "02:09:21 , Epoch: 0 [13568/22693], Loss: 0.18525521406271028 Accuracy: 0.265625\n",
      "02:09:21 , Epoch: 0 [13632/22693], Loss: 0.18521148387214942 Accuracy: 0.28125\n",
      "02:09:22 , Epoch: 0 [13696/22693], Loss: 0.1851691638079077 Accuracy: 0.265625\n",
      "02:09:23 , Epoch: 0 [13760/22693], Loss: 0.18508166837065224 Accuracy: 0.4375\n",
      "02:09:23 , Epoch: 0 [13824/22693], Loss: 0.1849770538125464 Accuracy: 0.296875\n",
      "02:09:24 , Epoch: 0 [13888/22693], Loss: 0.18510294186892035 Accuracy: 0.203125\n",
      "02:09:25 , Epoch: 0 [13952/22693], Loss: 0.1849883020404719 Accuracy: 0.34375\n",
      "02:09:25 , Epoch: 0 [14016/22693], Loss: 0.1849913612820104 Accuracy: 0.359375\n",
      "02:09:26 , Epoch: 0 [14080/22693], Loss: 0.18502725675130036 Accuracy: 0.234375\n",
      "02:09:27 , Epoch: 0 [14144/22693], Loss: 0.18493578239695854 Accuracy: 0.375\n",
      "02:09:28 , Epoch: 0 [14208/22693], Loss: 0.18487086447778905 Accuracy: 0.40625\n",
      "02:09:28 , Epoch: 0 [14272/22693], Loss: 0.18484324190351245 Accuracy: 0.328125\n",
      "02:09:29 , Epoch: 0 [14336/22693], Loss: 0.18483926248561747 Accuracy: 0.296875\n",
      "02:09:30 , Epoch: 0 [14400/22693], Loss: 0.1848457791658647 Accuracy: 0.25\n",
      "02:09:30 , Epoch: 0 [14464/22693], Loss: 0.1849334751546201 Accuracy: 0.21875\n",
      "02:09:31 , Epoch: 0 [14528/22693], Loss: 0.18489367444743013 Accuracy: 0.3125\n",
      "02:09:32 , Epoch: 0 [14592/22693], Loss: 0.18484515002236485 Accuracy: 0.3125\n",
      "02:09:32 , Epoch: 0 [14656/22693], Loss: 0.18470540991511247 Accuracy: 0.421875\n",
      "02:09:33 , Epoch: 0 [14720/22693], Loss: 0.18470012294932772 Accuracy: 0.296875\n",
      "02:09:34 , Epoch: 0 [14784/22693], Loss: 0.18465142136214235 Accuracy: 0.265625\n",
      "02:09:34 , Epoch: 0 [14848/22693], Loss: 0.18452185695578535 Accuracy: 0.375\n",
      "02:09:35 , Epoch: 0 [14912/22693], Loss: 0.18460223715256938 Accuracy: 0.203125\n",
      "02:09:36 , Epoch: 0 [14976/22693], Loss: 0.1845364251204539 Accuracy: 0.328125\n",
      "02:09:37 , Epoch: 0 [15040/22693], Loss: 0.18455050293648934 Accuracy: 0.28125\n",
      "02:09:37 , Epoch: 0 [15104/22693], Loss: 0.18455290178164593 Accuracy: 0.328125\n",
      "02:09:38 , Epoch: 0 [15168/22693], Loss: 0.18449143104785706 Accuracy: 0.21875\n",
      "02:09:39 , Epoch: 0 [15232/22693], Loss: 0.1843907149738864 Accuracy: 0.328125\n",
      "02:09:39 , Epoch: 0 [15296/22693], Loss: 0.18441027051483322 Accuracy: 0.21875\n",
      "02:09:40 , Epoch: 0 [15360/22693], Loss: 0.184319184962194 Accuracy: 0.421875\n",
      "02:09:41 , Epoch: 0 [15424/22693], Loss: 0.18426234069383124 Accuracy: 0.28125\n",
      "02:09:41 , Epoch: 0 [15488/22693], Loss: 0.18420985211048113 Accuracy: 0.375\n",
      "02:09:42 , Epoch: 0 [15552/22693], Loss: 0.1842362561697153 Accuracy: 0.265625\n",
      "02:09:43 , Epoch: 0 [15616/22693], Loss: 0.1841961117285564 Accuracy: 0.34375\n",
      "02:09:43 , Epoch: 0 [15680/22693], Loss: 0.18411291323525522 Accuracy: 0.359375\n",
      "02:09:44 , Epoch: 0 [15744/22693], Loss: 0.1839971346932102 Accuracy: 0.3125\n",
      "02:09:45 , Epoch: 0 [15808/22693], Loss: 0.1840014607139269 Accuracy: 0.265625\n",
      "02:09:45 , Epoch: 0 [15872/22693], Loss: 0.1839046630820837 Accuracy: 0.3125\n",
      "02:09:46 , Epoch: 0 [15936/22693], Loss: 0.1838599622504884 Accuracy: 0.296875\n",
      "02:09:47 , Epoch: 0 [16000/22693], Loss: 0.18372945306033336 Accuracy: 0.40625\n",
      "02:09:47 , Epoch: 0 [16064/22693], Loss: 0.18370452683902225 Accuracy: 0.375\n",
      "02:09:48 , Epoch: 0 [16128/22693], Loss: 0.18363080603827805 Accuracy: 0.359375\n",
      "02:09:49 , Epoch: 0 [16192/22693], Loss: 0.1836563931521076 Accuracy: 0.28125\n",
      "02:09:49 , Epoch: 0 [16256/22693], Loss: 0.18365697088740848 Accuracy: 0.3125\n",
      "02:09:50 , Epoch: 0 [16320/22693], Loss: 0.1835769087750466 Accuracy: 0.4375\n",
      "02:09:51 , Epoch: 0 [16384/22693], Loss: 0.18361190445702358 Accuracy: 0.234375\n",
      "02:09:51 , Epoch: 0 [16448/22693], Loss: 0.18359438239231113 Accuracy: 0.359375\n",
      "02:09:52 , Epoch: 0 [16512/22693], Loss: 0.18354161574268452 Accuracy: 0.296875\n",
      "02:09:53 , Epoch: 0 [16576/22693], Loss: 0.18350349757064482 Accuracy: 0.328125\n",
      "02:09:53 , Epoch: 0 [16640/22693], Loss: 0.18341745436068538 Accuracy: 0.328125\n",
      "02:09:54 , Epoch: 0 [16704/22693], Loss: 0.1833775969550744 Accuracy: 0.265625\n",
      "02:09:55 , Epoch: 0 [16768/22693], Loss: 0.18333140719973604 Accuracy: 0.3125\n",
      "02:09:55 , Epoch: 0 [16832/22693], Loss: 0.18322820349908622 Accuracy: 0.34375\n",
      "02:09:56 , Epoch: 0 [16896/22693], Loss: 0.18325421660406765 Accuracy: 0.234375\n",
      "02:09:57 , Epoch: 0 [16960/22693], Loss: 0.1832982164353748 Accuracy: 0.21875\n",
      "02:09:57 , Epoch: 0 [17024/22693], Loss: 0.18331691071626977 Accuracy: 0.34375\n",
      "02:09:58 , Epoch: 0 [17088/22693], Loss: 0.1832692822386735 Accuracy: 0.40625\n",
      "02:09:58 , Epoch: 0 [17152/22693], Loss: 0.1831811463845269 Accuracy: 0.296875\n",
      "02:09:59 , Epoch: 0 [17216/22693], Loss: 0.18311687691725706 Accuracy: 0.328125\n",
      "02:10:00 , Epoch: 0 [17280/22693], Loss: 0.18317692179999606 Accuracy: 0.265625\n",
      "02:10:00 , Epoch: 0 [17344/22693], Loss: 0.18313322484850564 Accuracy: 0.234375\n",
      "02:10:01 , Epoch: 0 [17408/22693], Loss: 0.18305525934739525 Accuracy: 0.296875\n",
      "02:10:02 , Epoch: 0 [17472/22693], Loss: 0.1830568340359748 Accuracy: 0.375\n",
      "02:10:02 , Epoch: 0 [17536/22693], Loss: 0.1830896129343294 Accuracy: 0.21875\n",
      "02:10:03 , Epoch: 0 [17600/22693], Loss: 0.18308308882359867 Accuracy: 0.359375\n",
      "02:10:04 , Epoch: 0 [17664/22693], Loss: 0.18307095494498424 Accuracy: 0.234375\n",
      "02:10:04 , Epoch: 0 [17728/22693], Loss: 0.18302938478724617 Accuracy: 0.28125\n",
      "02:10:05 , Epoch: 0 [17792/22693], Loss: 0.1829215661661879 Accuracy: 0.28125\n",
      "02:10:06 , Epoch: 0 [17856/22693], Loss: 0.18286457555136887 Accuracy: 0.265625\n",
      "02:10:06 , Epoch: 0 [17920/22693], Loss: 0.1827617954120658 Accuracy: 0.421875\n",
      "02:10:07 , Epoch: 0 [17984/22693], Loss: 0.18266799699417968 Accuracy: 0.390625\n",
      "02:10:08 , Epoch: 0 [18048/22693], Loss: 0.18252819979436744 Accuracy: 0.453125\n",
      "02:10:08 , Epoch: 0 [18112/22693], Loss: 0.1824978811618822 Accuracy: 0.34375\n",
      "02:10:09 , Epoch: 0 [18176/22693], Loss: 0.18256409328849404 Accuracy: 0.28125\n",
      "02:10:10 , Epoch: 0 [18240/22693], Loss: 0.18256361095044263 Accuracy: 0.3125\n",
      "02:10:10 , Epoch: 0 [18304/22693], Loss: 0.18254658119734168 Accuracy: 0.328125\n",
      "02:10:11 , Epoch: 0 [18368/22693], Loss: 0.18256462085876882 Accuracy: 0.28125\n",
      "02:10:12 , Epoch: 0 [18432/22693], Loss: 0.18256175848203787 Accuracy: 0.28125\n",
      "02:10:12 , Epoch: 0 [18496/22693], Loss: 0.18255605340379052 Accuracy: 0.265625\n",
      "02:10:13 , Epoch: 0 [18560/22693], Loss: 0.1825734539250386 Accuracy: 0.25\n",
      "02:10:13 , Epoch: 0 [18624/22693], Loss: 0.18263372426057117 Accuracy: 0.25\n",
      "02:10:14 , Epoch: 0 [18688/22693], Loss: 0.1825292321463047 Accuracy: 0.421875\n",
      "02:10:15 , Epoch: 0 [18752/22693], Loss: 0.18256770797644706 Accuracy: 0.328125\n",
      "02:10:15 , Epoch: 0 [18816/22693], Loss: 0.18251843385572467 Accuracy: 0.40625\n",
      "02:10:16 , Epoch: 0 [18880/22693], Loss: 0.18238839268924772 Accuracy: 0.390625\n",
      "02:10:17 , Epoch: 0 [18944/22693], Loss: 0.18235970004753377 Accuracy: 0.328125\n",
      "02:10:17 , Epoch: 0 [19008/22693], Loss: 0.18237113601995356 Accuracy: 0.203125\n",
      "02:10:18 , Epoch: 0 [19072/22693], Loss: 0.1823949033195458 Accuracy: 0.28125\n",
      "02:10:19 , Epoch: 0 [19136/22693], Loss: 0.18245225786955882 Accuracy: 0.390625\n",
      "02:10:19 , Epoch: 0 [19200/22693], Loss: 0.18240788781148115 Accuracy: 0.3125\n",
      "02:10:20 , Epoch: 0 [19264/22693], Loss: 0.18237962612931516 Accuracy: 0.21875\n",
      "02:10:21 , Epoch: 0 [19328/22693], Loss: 0.18240156147082343 Accuracy: 0.25\n",
      "02:10:21 , Epoch: 0 [19392/22693], Loss: 0.18236413296311912 Accuracy: 0.3125\n",
      "02:10:22 , Epoch: 0 [19456/22693], Loss: 0.1824071811394866 Accuracy: 0.140625\n",
      "02:10:23 , Epoch: 0 [19520/22693], Loss: 0.18239736965266762 Accuracy: 0.21875\n",
      "02:10:23 , Epoch: 0 [19584/22693], Loss: 0.18241401803619348 Accuracy: 0.265625\n",
      "02:10:24 , Epoch: 0 [19648/22693], Loss: 0.18241322783378464 Accuracy: 0.28125\n",
      "02:10:25 , Epoch: 0 [19712/22693], Loss: 0.18233254224977158 Accuracy: 0.296875\n",
      "02:10:25 , Epoch: 0 [19776/22693], Loss: 0.1823755957042176 Accuracy: 0.3125\n",
      "02:10:26 , Epoch: 0 [19840/22693], Loss: 0.18237150048142406 Accuracy: 0.265625\n",
      "02:10:27 , Epoch: 0 [19904/22693], Loss: 0.18230696182684775 Accuracy: 0.3125\n",
      "02:10:28 , Epoch: 0 [19968/22693], Loss: 0.18229588914713107 Accuracy: 0.3125\n",
      "02:10:28 , Epoch: 0 [20032/22693], Loss: 0.18226788702483462 Accuracy: 0.265625\n",
      "02:10:29 , Epoch: 0 [20096/22693], Loss: 0.18222068389175075 Accuracy: 0.296875\n",
      "02:10:30 , Epoch: 0 [20160/22693], Loss: 0.18223696616836388 Accuracy: 0.25\n",
      "02:10:30 , Epoch: 0 [20224/22693], Loss: 0.1822311966235945 Accuracy: 0.265625\n",
      "02:10:31 , Epoch: 0 [20288/22693], Loss: 0.1822491933396682 Accuracy: 0.171875\n",
      "02:10:32 , Epoch: 0 [20352/22693], Loss: 0.18222155412161167 Accuracy: 0.28125\n",
      "02:10:32 , Epoch: 0 [20416/22693], Loss: 0.1822341576036274 Accuracy: 0.21875\n",
      "02:10:33 , Epoch: 0 [20480/22693], Loss: 0.18219871645803776 Accuracy: 0.25\n",
      "02:10:34 , Epoch: 0 [20544/22693], Loss: 0.18211841176744256 Accuracy: 0.25\n",
      "02:10:34 , Epoch: 0 [20608/22693], Loss: 0.18206013010036162 Accuracy: 0.265625\n",
      "02:10:35 , Epoch: 0 [20672/22693], Loss: 0.1820055638173931 Accuracy: 0.328125\n",
      "02:10:36 , Epoch: 0 [20736/22693], Loss: 0.18194046679619513 Accuracy: 0.390625\n",
      "02:10:36 , Epoch: 0 [20800/22693], Loss: 0.1819571573648388 Accuracy: 0.25\n",
      "02:10:37 , Epoch: 0 [20864/22693], Loss: 0.1818851591447582 Accuracy: 0.28125\n",
      "02:10:38 , Epoch: 0 [20928/22693], Loss: 0.18190736911936595 Accuracy: 0.328125\n",
      "02:10:39 , Epoch: 0 [20992/22693], Loss: 0.1818570415486294 Accuracy: 0.296875\n",
      "02:10:39 , Epoch: 0 [21056/22693], Loss: 0.18186300813598982 Accuracy: 0.25\n",
      "02:10:40 , Epoch: 0 [21120/22693], Loss: 0.18176322404340922 Accuracy: 0.359375\n",
      "02:10:41 , Epoch: 0 [21184/22693], Loss: 0.18173390385847388 Accuracy: 0.34375\n",
      "02:10:42 , Epoch: 0 [21248/22693], Loss: 0.18164089853867757 Accuracy: 0.328125\n",
      "02:10:42 , Epoch: 0 [21312/22693], Loss: 0.1816409191628715 Accuracy: 0.3125\n",
      "02:10:43 , Epoch: 0 [21376/22693], Loss: 0.18156771364703334 Accuracy: 0.40625\n",
      "02:10:44 , Epoch: 0 [21440/22693], Loss: 0.18155672190415753 Accuracy: 0.234375\n",
      "02:10:44 , Epoch: 0 [21504/22693], Loss: 0.18151980239584398 Accuracy: 0.3125\n",
      "02:10:45 , Epoch: 0 [21568/22693], Loss: 0.18151348103662113 Accuracy: 0.265625\n",
      "02:10:46 , Epoch: 0 [21632/22693], Loss: 0.18144831837624711 Accuracy: 0.328125\n",
      "02:10:46 , Epoch: 0 [21696/22693], Loss: 0.18152405441673264 Accuracy: 0.21875\n",
      "02:10:47 , Epoch: 0 [21760/22693], Loss: 0.1815226889707515 Accuracy: 0.25\n",
      "02:10:48 , Epoch: 0 [21824/22693], Loss: 0.18149645209458667 Accuracy: 0.296875\n",
      "02:10:48 , Epoch: 0 [21888/22693], Loss: 0.18142468962339206 Accuracy: 0.359375\n",
      "02:10:49 , Epoch: 0 [21952/22693], Loss: 0.18137634942910252 Accuracy: 0.34375\n",
      "02:10:50 , Epoch: 0 [22016/22693], Loss: 0.18134937389377664 Accuracy: 0.3125\n",
      "02:10:50 , Epoch: 0 [22080/22693], Loss: 0.18131728305187964 Accuracy: 0.328125\n",
      "02:10:51 , Epoch: 0 [22144/22693], Loss: 0.18136947025747777 Accuracy: 0.21875\n",
      "02:10:52 , Epoch: 0 [22208/22693], Loss: 0.18137438951689974 Accuracy: 0.3125\n",
      "02:10:52 , Epoch: 0 [22272/22693], Loss: 0.18139553679618386 Accuracy: 0.296875\n",
      "02:10:53 , Epoch: 0 [22336/22693], Loss: 0.18131129343569083 Accuracy: 0.421875\n",
      "02:10:54 , Epoch: 0 [22400/22693], Loss: 0.18126058296543238 Accuracy: 0.3125\n",
      "02:10:55 , Epoch: 0 [22464/22693], Loss: 0.18131221066941688 Accuracy: 0.265625\n",
      "02:10:57 , Epoch: 0 [22528/22693], Loss: 0.18133454891941111 Accuracy: 0.25\n",
      "02:10:57 , Epoch: 0 [22592/22693], Loss: 0.18135367844415376 Accuracy: 0.25\n",
      "02:10:58 , Epoch: 0 [22656/22693], Loss: 0.18139596593867263 Accuracy: 0.265625\n",
      "Epoch: 0 Validation Loss: 0.1605426816562082\n",
      "Epoch: 0 Validation Loss: 0.09228959410335123\n",
      "Epoch: 0 Validation Loss: 0.05610122934967356\n",
      "Epoch: 0 Validation Loss: 0.052884132379632874\n",
      "Epoch: 0 Validation Loss: 0.035457388659747\n",
      "Epoch: 0 Validation Loss: 0.028737644835473017\n",
      "Epoch: 0 Validation Loss: 0.021880012034973605\n",
      "Epoch: 0 Validation Loss: 0.022211482778122127\n",
      "Epoch: 0 Validation Loss: 0.020459806954730743\n",
      "Epoch: 0 Validation Loss: 0.019812604712026283\n",
      "Epoch: 0 Validation Loss: 0.014882538844126091\n",
      "Epoch: 0 Validation Loss: 0.015725447739077702\n",
      "Epoch: 0 Validation Loss: 0.013874273581238218\n",
      "Epoch: 0 Validation Loss: 0.013569442823941211\n",
      "Epoch: 0 Validation Loss: 0.012961152642959517\n",
      "Epoch: 0 Validation Loss: 0.01043247901050366\n",
      "Epoch: 0 Validation Loss: 0.011929991078240387\n",
      "Epoch: 0 Validation Loss: 0.01099707575925802\n",
      "Epoch: 0 Validation Loss: 0.010091946123106682\n",
      "Epoch: 0 Validation Loss: 0.008758724215952095\n",
      "Epoch: 0 Validation Loss: 0.008101144665009701\n",
      "Epoch: 0 Validation Loss: 0.007873698197844002\n",
      "Epoch: 0 Validation Loss: 0.009854736485768168\n",
      "Epoch: 0 Validation Loss: 0.007673017843736704\n",
      "Epoch: 0 Validation Loss: 0.0062608969476192974\n",
      "Epoch: 0 Validation Loss: 0.006672396643263258\n",
      "Epoch: 0 Validation Loss: 0.005798096944286274\n",
      "Epoch: 0 Validation Loss: 0.00836434887224309\n",
      "Epoch: 0 Validation Loss: 0.006716736937384177\n",
      "Epoch: 0 Validation Loss: 0.006262802618611851\n",
      "Epoch: 0 Validation Loss: 0.005993366704331328\n",
      "Epoch: 0 Validation Loss: 0.005412838443674605\n",
      "Epoch: 0 Validation Loss: 0.005242417680763755\n",
      "Epoch: 0 Validation Loss: 0.005891401489871508\n",
      "Epoch: 0 Validation Loss: 0.004773849389349295\n",
      "Epoch: 0 Validation Loss: 0.004911905436217088\n",
      "Epoch: 0 Validation Loss: 0.004658927628455535\n",
      "Epoch: 0 Validation Loss: 0.004610698242190426\n",
      "Epoch: 0 Validation Loss: 0.004549355461042885\n",
      "Epoch: 0 Validation Loss: 0.004616808698925259\n",
      "Epoch: 0 Validation Loss: 0.0050524209198513125\n",
      "Epoch: 0 Validation Loss: 0.004116512499577564\n",
      "Epoch: 0 Validation Loss: 0.0041811500604272455\n",
      "Epoch: 0 Validation Loss: 0.004409265237041406\n",
      "Epoch: 0 Validation Loss: 0.00371954202477292\n",
      "Epoch: 0 Validation Loss: 0.0040937915414373995\n",
      "Epoch: 0 Validation Loss: 0.003705643531633231\n",
      "Epoch: 0 Validation Loss: 0.003532159870239373\n",
      "Epoch: 0 Validation Loss: 0.0045438377729949755\n",
      "Epoch: 0 Validation Loss: 0.0038986750247905462\n",
      "Epoch: 0 Validation Loss: 0.0035025231996985017\n",
      "Epoch: 0 Validation Loss: 0.00370880683164172\n",
      "Epoch: 0 Validation Loss: 0.0032163071344746934\n",
      "Epoch: 0 Validation Loss: 0.003328594326794912\n",
      "Epoch: 0 Validation Loss: 0.0030525574821538165\n",
      "Epoch: 0 Validation Loss: 0.003675652962947074\n",
      "Epoch: 0 Validation Loss: 0.003106367457843039\n",
      "Epoch: 0 Validation Loss: 0.003269588413581812\n",
      "Epoch: 0 Validation Loss: 0.0034798264921908247\n",
      "Epoch: 0 Validation Loss: 0.003005614377515723\n",
      "Epoch: 0 Validation Loss: 0.002803476233988252\n",
      "Epoch: 0 Validation Loss: 0.0032486141197523386\n",
      "Epoch: 0 Validation Loss: 0.0025054072470014677\n",
      "Epoch: 0 Validation Loss: 0.003190357397963372\n",
      "Epoch: 0 Validation Loss: 0.0029170277797472945\n",
      "Epoch: 0 Validation Loss: 0.0028508767870637653\n",
      "Epoch: 0 Validation Loss: 0.002981382874734302\n",
      "Epoch: 0 Validation Loss: 0.002954349639882701\n",
      "Epoch: 0 Validation Loss: 0.0029538726472857916\n",
      "Epoch: 0 Validation Loss: 0.0027777520012489624\n",
      "Epoch: 0 Validation Loss: 0.0023966608931990764\n",
      "Epoch: 0 Validation Loss: 0.0024843893653178533\n",
      "Epoch: 0 Validation Loss: 0.0023283861337155655\n",
      "Epoch: 0 Validation Loss: 0.002600807436429242\n",
      "Epoch: 0 Validation Loss: 0.0027863538855689217\n",
      "Epoch: 0 Validation Loss: 0.0024967926034777893\n",
      "Epoch: 0 Validation Loss: 0.0024031714988029438\n",
      "Epoch: 0 Validation Loss: 0.0022328286023733427\n",
      "Epoch: 0 Validation Loss: 0.0022109209560585118\n",
      "Epoch: 0 Validation Loss: 0.002458880327648914\n",
      "Epoch: 0 Validation Loss: 0.002343407008024309\n",
      "Epoch: 0 Validation Loss: 0.0025328755898074043\n",
      "Epoch: 0 Validation Loss: 0.002451772133774025\n",
      "Epoch: 0 Validation Loss: 0.0020325635508878235\n",
      "Epoch: 0 Validation Loss: 0.002073051855108272\n",
      "Epoch: 0 Validation Loss: 0.0022974863946767466\n",
      "Epoch: 0 Validation Loss: 0.0020281073939658516\n",
      "Epoch: 0 Validation Loss: 0.0022511303613733744\n",
      "Epoch: 0 Validation Loss: 0.0021149739513186714\n",
      "Epoch: 0 Validation Loss: 0.0016860667086480263\n",
      "Epoch: 0 Validation Loss: 0.001679341054870134\n",
      "Epoch: 0 Validation Loss: 0.0018069588746644038\n",
      "Epoch: 0 Validation Loss: 0.0020220369023543614\n",
      "Epoch: 0 Validation Loss: 0.0018192113060368758\n",
      "Epoch: 0 Validation Loss: 0.0016262082641665813\n",
      "Epoch: 0 Validation Loss: 0.0016820291306296812\n",
      "Epoch: 0 Validation Loss: 0.0016676594921014392\n",
      "Epoch: 0 Validation Loss: 0.0016614881807508457\n",
      "Epoch: 0 Validation Loss: 0.0017335657157401157\n",
      "Epoch: 0 Validation Loss: 0.0020130408808085973\n",
      "Epoch: 0 Validation Loss: 0.0018233925793440346\n",
      "Epoch: 0 Validation Loss: 0.0017887911038585065\n",
      "Epoch: 0 Validation Loss: 0.0020763634296750893\n",
      "Epoch: 0 Validation Loss: 0.0014319181090794442\n",
      "Epoch: 0 Validation Loss: 0.0017124046388775627\n",
      "Epoch: 0 Validation Loss: 0.001654479353573307\n",
      "Epoch: 0 Validation Loss: 0.0017592200196171698\n",
      "Epoch: 0 Validation Loss: 0.0014877870913018213\n",
      "Epoch: 0 Validation Loss: 0.002253116395010455\n",
      "Epoch: 0 Validation Loss: 0.0018641136274028186\n",
      "Epoch: 0 Validation Loss: 0.0017261175404645354\n",
      "Epoch: 0 Validation Loss: 0.0015663572442972864\n",
      "Epoch: 0 Validation Loss: 0.0016953632139101468\n",
      "Epoch: 0 Validation Loss: 0.0013507957325266024\n",
      "Epoch: 0 Validation Loss: 0.0017130257368535395\n",
      "Epoch: 0 Validation Loss: 0.0015154611359372123\n",
      "Epoch: 0 Validation Loss: 0.0013816705657039535\n",
      "F-MEASURE (0.35): 0.3641968069007263 ACCURACY: 0.2567531779661017\n",
      "F-measure History:  [0.36419681]\n",
      "Accuracy History:  [0.25675318]\n"
     ]
    }
   ],
   "source": [
    "# Using a function to apply 1d convolution where kernel_size=7\n",
    "def conv3x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=7, stride=stride,\n",
    "                     padding=3, bias=False)\n",
    "\n",
    "# Using a function to apply 1d convolution where kernel_size=1\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "# Residual Block\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x1(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x1(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "\n",
    "    # Feed data through the network\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Bottleneck layer to reduce overfitting\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = conv3x1(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "\n",
    "    # Feed data through the network\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# ResNet model (defining architecture)\n",
    "class ResNet(nn.Module):\n",
    "    # in_channel=number of leads, out_channel=number of classes\n",
    "    def __init__(self, block, layers, in_channel=12, out_channel=27, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64 # Batch size\n",
    "        self.conv1 = nn.Conv1d(in_channel, 64, kernel_size=5000, stride=2, padding=7,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, out_channel)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    # Feed data through the network\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# ResNet18 model\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "# ResNet34 model\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "# Training ResNet\n",
    "def train(data, labels, validationleads, validationlabels, epochnum, threshval, opt, resnettype):\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "\n",
    "    # Batch size\n",
    "    batchval=64\n",
    "    batchind = np.array(range(0, data.shape[0], batchval))\n",
    "    #Initialise Model\n",
    "    if resnettype == 18:\n",
    "        model = resnet18().double()\n",
    "    elif resnettype == 34:\n",
    "        model = resnet34().double()\n",
    "    model.cuda()\n",
    "    # Array to store trained models\n",
    "    models = []\n",
    "    # Optimiser\n",
    "    if opt == 0:\n",
    "        optimiser = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=1e-5)\n",
    "    elif opt == 1:\n",
    "        optimiser = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Initialise loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # Model Save path\n",
    "    model_save_path = \"torch_models/\"\n",
    "    # Ndarray to store predicted labels\n",
    "    pred_labels = np.zeros(shape=labels.shape)\n",
    "    # Used to store models based on f-measure values\n",
    "    prev_f_measure_val = 0.0\n",
    "    # Arrays to store history of results\n",
    "    f_measure_hist = np.ones(epochnum)\n",
    "    training_loss_hist = []\n",
    "    validation_loss_hist = []\n",
    "    training_acc_hist = []\n",
    "    validation_acc_hist = np.ones(epochnum)\n",
    "    loss_batches = 0.0\n",
    "    # Training phase\n",
    "    for epoch in range(0, epochnum):\n",
    "        model.train()\n",
    "        print(\"TRAINING EPOCH {}\".format(epoch))\n",
    "        batchcounter = 0\n",
    "        loss_epoch = 0.0\n",
    "        loss = 0.0\n",
    "        pred_labels = np.zeros(shape=labels.shape)\n",
    "        epoch_training_loss = []\n",
    "        # For each epoch, train on the training set\n",
    "        with torch.set_grad_enabled(True):\n",
    "            for i in batchind[:-1]:\n",
    "                # training data - Numpy to Tensor\n",
    "                x = torch.from_numpy(data[i:i+batchval,:,:] / 1000)\n",
    "                x = x.cuda()\n",
    "                \n",
    "                # training labels - Numpy to Tensor\n",
    "                trainlabels = torch.from_numpy(labels[i:i+batchval,:]).double()\n",
    "                trainlabels = trainlabels.to(device=dev)\n",
    "                \n",
    "                # Reset gradients\n",
    "                optimiser.zero_grad()\n",
    "                \n",
    "                # Make prediction\n",
    "                y = model(x).to(dev)\n",
    "                # Apply output layer function to predictions\n",
    "                y_prob = torch.sigmoid(y)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(y, trainlabels)\n",
    "                temploss = loss.item() * x.size(0)\n",
    "                loss_epoch += temploss\n",
    "                \n",
    "                # Backwards step\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss_batches += temploss\n",
    "                batchcounter += x.size(0)\n",
    "                current_loss = loss_batches / batchcounter\n",
    "\n",
    "                # Store predictions and labels\n",
    "                if i == 0:\n",
    "                    all_train_labels = trainlabels\n",
    "                    all_pred_prob = y_prob\n",
    "                else:\n",
    "                    all_train_labels = torch.cat((all_train_labels, trainlabels), 0)\n",
    "                    all_pred_prob = torch.cat((all_pred_prob, y_prob), 0)\n",
    "\n",
    "                # Covert predictions from tensor to numpy\n",
    "                pred_labels[i:i+batchval,:] = y_prob.cpu().detach().numpy()\n",
    "\n",
    "                # Apply threshold to predictions to recieve binary output\n",
    "                pred_labels[pred_labels < threshval] = 0\n",
    "                pred_labels[pred_labels > threshval] = 1\n",
    "\n",
    "                # Calculate Training Accuracy\n",
    "                training_acc = accuracy_score(labels[i:i+batchval,:], pred_labels[i:i+batchval,:])\n",
    "\n",
    "                # Store Loss\n",
    "                epoch_training_loss.append(current_loss)\n",
    "\n",
    "                # Output current training information\n",
    "                print(\"{}\".format(time.strftime(\"%H:%M:%S\", time.localtime())), \", Epoch: {} [{}/{}],\".format(epoch,i+batchval,data.shape[0]), \n",
    "                \"Loss: {}\".format(current_loss), \"Accuracy: {}\".format(training_acc))\n",
    "            \n",
    "            training_acc_hist.append(training_acc)\n",
    "            training_loss_hist.append(epoch_training_loss)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        conf_mat_train = multilabel_confusion_matrix(labels[:pred_labels.shape[0],:], pred_labels)\n",
    "        class_report = classification_report(labels[:pred_labels.shape[0],:], pred_labels)\n",
    "    \n",
    "\n",
    "\n",
    "        # Evaluation mode (using validation set)\n",
    "        model.eval()\n",
    "        # Batch size\n",
    "        val_batchval=64\n",
    "        batchindval = np.array(range(0, validationleads.shape[0], val_batchval))\n",
    "        valid_loss = 0.0 # Loss variable\n",
    "        batchcounterval = 0 # batch counter variable\n",
    "        val_pred_labels = np.zeros(shape=validationlabels.shape)\n",
    "        epoch_validation_loss = []\n",
    "        # Initialise loss function\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for j in batchindval[:-1]:\n",
    "                # validation leads - numpy to tensor\n",
    "                x_val = torch.from_numpy(validationleads[j:j+val_batchval,:,:] / 1000)\n",
    "                x_val = x_val.cuda()\n",
    "                # validation labels - numpy to tensor\n",
    "                val_labels = torch.from_numpy(validationlabels[j:j+val_batchval,:]).double().cuda()\n",
    "                val_labels = val_labels.to(device=dev)\n",
    "\n",
    "                # Make prediction\n",
    "                val_y = model(x_val).to(device=dev)\n",
    "                # Apply output layer function\n",
    "                val_y_prob = torch.sigmoid(val_y)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss = criterion(val_y, val_labels)\n",
    "                valid_loss = val_loss.item() * x_val.size(0)\n",
    "                batchcounterval += x_val.size(0)\n",
    "                current_val_loss = valid_loss / batchcounterval\n",
    "\n",
    "                # Store raw predictions\n",
    "                if j == 0:\n",
    "                    all_val_labels = val_labels\n",
    "                    all_val_prob = val_y_prob\n",
    "                else:\n",
    "                    all_train_labels = torch.cat((all_val_labels, val_labels), 0)\n",
    "                    all_pred_prob = torch.cat((all_val_prob, val_y_prob), 0)\n",
    "\n",
    "                # Output predicted values from tensor to numpy\n",
    "                val_pred_labels[j:j+val_batchval,:] = y_prob.cpu().detach().numpy()\n",
    "\n",
    "                # Ouput current validation loss\n",
    "                epoch_validation_loss.append(current_val_loss)\n",
    "                print(\"Epoch: {}\".format(epoch), \"Validation Loss: {}\".format(current_val_loss))\n",
    "        \n",
    "        # Calculating and Storing f-measure and validation accuracy\n",
    "        validation_loss_hist.append(epoch_validation_loss)\n",
    "        models.append(model)\n",
    "        \n",
    "        tempfmeasure = []\n",
    "        tempaccmeasure = []\n",
    "\n",
    "\n",
    "        val_pred_labels[val_pred_labels > threshval] = 1\n",
    "        val_pred_labels[val_pred_labels < threshval] = 0\n",
    "        \n",
    "        val_pred_labels = val_pred_labels.astype(int)\n",
    "        f_measure_val = f1_score(validationlabels, val_pred_labels, average='samples')\n",
    "        validation_acc = accuracy_score(validationlabels, val_pred_labels)\n",
    "        tempfmeasure.append(f_measure_val)\n",
    "        tempaccmeasure.append(validation_acc)\n",
    "\n",
    "        validation_acc_hist[epoch] = validation_acc\n",
    "\n",
    "        f_measure_hist[epoch] = f_measure_val\n",
    "\n",
    "        print(\"F-MEASURE ({}): {}\".format(threshval, f_measure_val), \"ACCURACY: {}\".format(validation_acc))\n",
    "\n",
    "        # Save model to path\n",
    "        if f_measure_val > prev_f_measure_val:\n",
    "            newmodelpath = model_save_path + \"model_e\" + str(epoch) + \"_f\" + str(f_measure_val) + \".pth\"\n",
    "            torch.save(model.state_dict(), newmodelpath)\n",
    "            prev_f_measure_val = f_measure_val\n",
    "\n",
    "    # print(challenge_metric_hist)    \n",
    "    print(\"F-measure History: \", f_measure_hist)\n",
    "    print(\"Accuracy History: \", validation_acc_hist)\n",
    "\n",
    "    return training_loss_hist, validation_loss_hist, f_measure_hist, validation_acc_hist, conf_mat_train, class_report, models\n",
    "\n",
    "\n",
    "\n",
    "training_loss_hist, validation_loss_hist, f_measure_hist, validation_acc_hist, conf_mat_train, class_report, models = train(trainingleads, traininglabels, validationleads, validationlabels, epochnum=1, threshval=0.35, opt=0, resnettype=34)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1418\n",
      "           1       0.14      0.00      0.00      2062\n",
      "           2       0.00      0.00      0.00       181\n",
      "           3       0.00      0.00      0.00       188\n",
      "           4       0.00      0.00      0.00       399\n",
      "           5       0.00      0.00      0.00      1009\n",
      "           6       0.27      0.00      0.01      1111\n",
      "           7       0.17      0.00      0.00      3700\n",
      "           8       0.17      0.01      0.02       627\n",
      "           9       0.00      0.00      0.00       334\n",
      "          10       0.00      0.00      0.00       603\n",
      "          11       0.00      0.00      0.00       178\n",
      "          12       0.00      0.00      0.00      1041\n",
      "          13       0.00      0.00      0.00       126\n",
      "          14       0.00      0.00      0.00       212\n",
      "          15       0.00      0.00      0.00       918\n",
      "          16       0.00      0.00      0.00       599\n",
      "          17       0.00      0.00      0.00       229\n",
      "          18       0.00      0.00      0.00      1454\n",
      "          19       0.00      0.00      0.00       752\n",
      "          20       0.00      0.00      0.00      1427\n",
      "          21       0.55      0.92      0.69     12456\n",
      "          22       0.00      0.00      0.00      1460\n",
      "          23       0.00      0.00      0.00       125\n",
      "          24       0.14      0.00      0.00      2837\n",
      "          25       0.00      0.00      0.00       686\n",
      "          26       0.00      0.00      0.00       228\n",
      "\n",
      "   micro avg       0.55      0.32      0.40     36360\n",
      "   macro avg       0.05      0.03      0.03     36360\n",
      "weighted avg       0.24      0.32      0.24     36360\n",
      " samples avg       0.51      0.38      0.42     36360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_mat_train.shape\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.0625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.078125\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.09375\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.0625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.0625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.0625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.015625\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.03125\n",
      "F-MEASURE (0.3): 0.109375\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.0625\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.0625\n",
      "F-MEASURE (0.3): 0.0\n",
      "F-MEASURE (0.3): 0.046875\n",
      "F-MEASURE (0.3): 0.078125\n",
      "0.3780828161036494\n"
     ]
    }
   ],
   "source": [
    "def test_eval(model, testingleads, testinglabels, threshval):\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        model.cuda()\n",
    "        # Batch size\n",
    "        batchval=64\n",
    "        batchind = np.array(range(0, testingleads.shape[0], batchval))\n",
    "        test_pred_labels = np.zeros(shape=testinglabels.shape)\n",
    "\n",
    "        for i in batchind[:-1]:\n",
    "            # testing leads - numpy to tensor\n",
    "            x = torch.from_numpy(testingleads[i:i+batchval,:,:] / 1000)\n",
    "            x = x.cuda()\n",
    "\n",
    "            # testing labels - numpy to tensor\n",
    "            test_labels = torch.from_numpy(testinglabels[i:i+batchval,:]).double().cuda()\n",
    "\n",
    "            # Make prediction\n",
    "            y = model(x).to(device=dev)\n",
    "            # Apply output layer function\n",
    "            test_y_prob = torch.sigmoid(y)\n",
    "\n",
    "            # Store predictions\n",
    "            if i == 0:\n",
    "                all_test_labels = test_labels\n",
    "                all_test_pred_prob = test_y_prob\n",
    "            else:\n",
    "                all_test_labels = torch.cat((all_test_labels, test_labels), 0)\n",
    "                all_test_pred_prob = torch.cat((all_test_pred_prob, test_y_prob), 0)\n",
    "\n",
    "            # Get predicted labels in numpy format\n",
    "            test_pred_labels[i:i+batchval,:] = test_y_prob.cpu().detach().numpy()\n",
    "\n",
    "            test_pred_labels = test_pred_labels.astype(int)\n",
    "\n",
    "            test_pred_labels[test_pred_labels > threshval] = 1\n",
    "            test_pred_labels[test_pred_labels < threshval] = 0\n",
    "\n",
    "            # Calculate Accuracy\n",
    "            f_measure_temp_val = accuracy_score(testinglabels[i:i+batchval,:], test_pred_labels[i:i+batchval,:])\n",
    "                \n",
    "            # Print testing accuracy per batch\n",
    "            print(\"Accuracy: {}\".format(f_measure_temp_val))\n",
    "        finalpred = all_test_pred_prob.cpu().detach().numpy()\n",
    "        finalpred[finalpred > threshval] = 1\n",
    "        finalpred[finalpred < threshval] = 0\n",
    "\n",
    "        # Calculate weighted f-measure\n",
    "        FMEASUREVAL = f1_score(testinglabels[:all_test_pred_prob.shape[0],:], finalpred, average='samples')\n",
    "        # Ouput confusion matrix and classification report\n",
    "        conf_mat_test = multilabel_confusion_matrix(testinglabels[:finalpred.shape[0],:], finalpred)\n",
    "        test_class_report = classification_report(testinglabels[:finalpred.shape[0],:], finalpred)\n",
    "        print(FMEASUREVAL)\n",
    "\n",
    "        return conf_mat_test, test_class_report\n",
    "\n",
    "\n",
    "testingmodel = models[0]\n",
    "\n",
    "conf_mat_test, test_class_report = test_eval(testingmodel, testingleads, testinglabels, threshval=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12, 5000])\n",
      "----------------------------------------------------------------------------\n",
      "           Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "               Conv1d-1         [64, 64, 8]       3,840,000       3,840,000\n",
      "          BatchNorm1d-2         [64, 64, 8]             128             128\n",
      "                 ReLU-3         [64, 64, 8]               0               0\n",
      "            MaxPool1d-4         [64, 64, 4]               0               0\n",
      "               Conv1d-5         [64, 64, 4]          28,672          28,672\n",
      "          BatchNorm1d-6         [64, 64, 4]             128             128\n",
      "                 ReLU-7         [64, 64, 4]               0               0\n",
      "              Dropout-8         [64, 64, 4]               0               0\n",
      "               Conv1d-9         [64, 64, 4]          28,672          28,672\n",
      "         BatchNorm1d-10         [64, 64, 4]             128             128\n",
      "              Conv1d-11         [64, 64, 4]          28,672          28,672\n",
      "         BatchNorm1d-12         [64, 64, 4]             128             128\n",
      "                ReLU-13         [64, 64, 4]               0               0\n",
      "             Dropout-14         [64, 64, 4]               0               0\n",
      "              Conv1d-15         [64, 64, 4]          28,672          28,672\n",
      "         BatchNorm1d-16         [64, 64, 4]             128             128\n",
      "              Conv1d-17        [64, 128, 2]          57,344          57,344\n",
      "         BatchNorm1d-18        [64, 128, 2]             256             256\n",
      "                ReLU-19        [64, 128, 2]               0               0\n",
      "             Dropout-20        [64, 128, 2]               0               0\n",
      "              Conv1d-21        [64, 128, 2]         114,688         114,688\n",
      "         BatchNorm1d-22        [64, 128, 2]             256             256\n",
      "              Conv1d-23        [64, 128, 2]           8,192           8,192\n",
      "         BatchNorm1d-24        [64, 128, 2]             256             256\n",
      "              Conv1d-25        [64, 128, 2]         114,688         114,688\n",
      "         BatchNorm1d-26        [64, 128, 2]             256             256\n",
      "                ReLU-27        [64, 128, 2]               0               0\n",
      "             Dropout-28        [64, 128, 2]               0               0\n",
      "              Conv1d-29        [64, 128, 2]         114,688         114,688\n",
      "         BatchNorm1d-30        [64, 128, 2]             256             256\n",
      "              Conv1d-31        [64, 256, 1]         229,376         229,376\n",
      "         BatchNorm1d-32        [64, 256, 1]             512             512\n",
      "                ReLU-33        [64, 256, 1]               0               0\n",
      "             Dropout-34        [64, 256, 1]               0               0\n",
      "              Conv1d-35        [64, 256, 1]         458,752         458,752\n",
      "         BatchNorm1d-36        [64, 256, 1]             512             512\n",
      "              Conv1d-37        [64, 256, 1]          32,768          32,768\n",
      "         BatchNorm1d-38        [64, 256, 1]             512             512\n",
      "              Conv1d-39        [64, 256, 1]         458,752         458,752\n",
      "         BatchNorm1d-40        [64, 256, 1]             512             512\n",
      "                ReLU-41        [64, 256, 1]               0               0\n",
      "             Dropout-42        [64, 256, 1]               0               0\n",
      "              Conv1d-43        [64, 256, 1]         458,752         458,752\n",
      "         BatchNorm1d-44        [64, 256, 1]             512             512\n",
      "              Conv1d-45        [64, 512, 1]         917,504         917,504\n",
      "         BatchNorm1d-46        [64, 512, 1]           1,024           1,024\n",
      "                ReLU-47        [64, 512, 1]               0               0\n",
      "             Dropout-48        [64, 512, 1]               0               0\n",
      "              Conv1d-49        [64, 512, 1]       1,835,008       1,835,008\n",
      "         BatchNorm1d-50        [64, 512, 1]           1,024           1,024\n",
      "              Conv1d-51        [64, 512, 1]         131,072         131,072\n",
      "         BatchNorm1d-52        [64, 512, 1]           1,024           1,024\n",
      "              Conv1d-53        [64, 512, 1]       1,835,008       1,835,008\n",
      "         BatchNorm1d-54        [64, 512, 1]           1,024           1,024\n",
      "                ReLU-55        [64, 512, 1]               0               0\n",
      "             Dropout-56        [64, 512, 1]               0               0\n",
      "              Conv1d-57        [64, 512, 1]       1,835,008       1,835,008\n",
      "         BatchNorm1d-58        [64, 512, 1]           1,024           1,024\n",
      "   AdaptiveAvgPool1d-59        [64, 512, 1]               0               0\n",
      "              Linear-60            [64, 27]          13,851          13,851\n",
      "============================================================================\n",
      "Total params: 12,579,739\n",
      "Trainable params: 12,579,739\n",
      "Non-trainable params: 0\n",
      "Batch size: 64\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'----------------------------------------------------------------------------\\n           Layer (type)        Output Shape         Param #     Tr. Param #\\n============================================================================\\n               Conv1d-1         [64, 64, 8]       3,840,000       3,840,000\\n          BatchNorm1d-2         [64, 64, 8]             128             128\\n                 ReLU-3         [64, 64, 8]               0               0\\n            MaxPool1d-4         [64, 64, 4]               0               0\\n               Conv1d-5         [64, 64, 4]          28,672          28,672\\n          BatchNorm1d-6         [64, 64, 4]             128             128\\n                 ReLU-7         [64, 64, 4]               0               0\\n              Dropout-8         [64, 64, 4]               0               0\\n               Conv1d-9         [64, 64, 4]          28,672          28,672\\n         BatchNorm1d-10         [64, 64, 4]             128             128\\n              Conv1d-11         [64, 64, 4]          28,672          28,672\\n         BatchNorm1d-12         [64, 64, 4]             128             128\\n                ReLU-13         [64, 64, 4]               0               0\\n             Dropout-14         [64, 64, 4]               0               0\\n              Conv1d-15         [64, 64, 4]          28,672          28,672\\n         BatchNorm1d-16         [64, 64, 4]             128             128\\n              Conv1d-17        [64, 128, 2]          57,344          57,344\\n         BatchNorm1d-18        [64, 128, 2]             256             256\\n                ReLU-19        [64, 128, 2]               0               0\\n             Dropout-20        [64, 128, 2]               0               0\\n              Conv1d-21        [64, 128, 2]         114,688         114,688\\n         BatchNorm1d-22        [64, 128, 2]             256             256\\n              Conv1d-23        [64, 128, 2]           8,192           8,192\\n         BatchNorm1d-24        [64, 128, 2]             256             256\\n              Conv1d-25        [64, 128, 2]         114,688         114,688\\n         BatchNorm1d-26        [64, 128, 2]             256             256\\n                ReLU-27        [64, 128, 2]               0               0\\n             Dropout-28        [64, 128, 2]               0               0\\n              Conv1d-29        [64, 128, 2]         114,688         114,688\\n         BatchNorm1d-30        [64, 128, 2]             256             256\\n              Conv1d-31        [64, 256, 1]         229,376         229,376\\n         BatchNorm1d-32        [64, 256, 1]             512             512\\n                ReLU-33        [64, 256, 1]               0               0\\n             Dropout-34        [64, 256, 1]               0               0\\n              Conv1d-35        [64, 256, 1]         458,752         458,752\\n         BatchNorm1d-36        [64, 256, 1]             512             512\\n              Conv1d-37        [64, 256, 1]          32,768          32,768\\n         BatchNorm1d-38        [64, 256, 1]             512             512\\n              Conv1d-39        [64, 256, 1]         458,752         458,752\\n         BatchNorm1d-40        [64, 256, 1]             512             512\\n                ReLU-41        [64, 256, 1]               0               0\\n             Dropout-42        [64, 256, 1]               0               0\\n              Conv1d-43        [64, 256, 1]         458,752         458,752\\n         BatchNorm1d-44        [64, 256, 1]             512             512\\n              Conv1d-45        [64, 512, 1]         917,504         917,504\\n         BatchNorm1d-46        [64, 512, 1]           1,024           1,024\\n                ReLU-47        [64, 512, 1]               0               0\\n             Dropout-48        [64, 512, 1]               0               0\\n              Conv1d-49        [64, 512, 1]       1,835,008       1,835,008\\n         BatchNorm1d-50        [64, 512, 1]           1,024           1,024\\n              Conv1d-51        [64, 512, 1]         131,072         131,072\\n         BatchNorm1d-52        [64, 512, 1]           1,024           1,024\\n              Conv1d-53        [64, 512, 1]       1,835,008       1,835,008\\n         BatchNorm1d-54        [64, 512, 1]           1,024           1,024\\n                ReLU-55        [64, 512, 1]               0               0\\n             Dropout-56        [64, 512, 1]               0               0\\n              Conv1d-57        [64, 512, 1]       1,835,008       1,835,008\\n         BatchNorm1d-58        [64, 512, 1]           1,024           1,024\\n   AdaptiveAvgPool1d-59        [64, 512, 1]               0               0\\n              Linear-60            [64, 27]          13,851          13,851\\n============================================================================\\nTotal params: 12,579,739\\nTrainable params: 12,579,739\\nNon-trainable params: 0\\nBatch size: 64\\n----------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.from_numpy(trainingleads[0:64,:,:]).cuda()\n",
    "print(x.shape)\n",
    "y = testingmodel(x)\n",
    "pms.summary(testingmodel, x, max_depth=None, show_input=False, batch_size=64, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 354)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Batch Number')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xlist = np.array(training_loss_hist)\n",
    "print(xlist.shape)\n",
    "\n",
    "plt.plot(xlist[0,:])\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(path, testingleads, testinglabels):\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        model = resnet18().double()\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "\n",
    "        batchval=64\n",
    "        batchind = np.array(range(0, testingleads.shape[0], batchval))\n",
    "\n",
    "        for i in batchind[:-1]:\n",
    "\n",
    "            x = torch.from_numpy(testingleads[i:i+batchval,:,:])\n",
    "            x = x.cuda()\n",
    "\n",
    "            test_pred_label = model(x)\n",
    "            test_y_prob = torch.sigmoid(test_pred_label)\n",
    "\n",
    "            testinglabels = torch.from_numpy(testinglabels[i:i+batchval,:]).double()\n",
    "            testinglabels = testinglabels.to(device=dev)\n",
    "\n",
    "            if i == 0:\n",
    "                all_test_labels = testinglabels\n",
    "                all_test_pred_prob = test_y_prob\n",
    "            else:\n",
    "                all_test_labels = torch.cat((all_test_labels, testinglabels), 0)\n",
    "                all_test_pred_prob = torch.cat((all_test_pred_prob, test_y_prob), 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(true_labels, pred_prob, threshold, weights, weightindex):\n",
    "    true_labels = true_labels.cpu().detach().numpy().astype(int)\n",
    "    labels = np.zeros(true_labels.shape)\n",
    "\n",
    "    _, pred_label = torch.max(pred_prob, 1)\n",
    "    pred_label = torch.unsqueeze(pred_label, 1)\n",
    "    pred_label = pred_label.cpu().detach().numpy()\n",
    "\n",
    "    labels[np.arange(true_labels.shape[0]), pred_label] = 1\n",
    "    y_prob = pred_prob.cpu().detach().numpy()\n",
    "    pred_prob = pred_prob.cpu().detach().numpy() >= threshold\n",
    "\n",
    "    labels = labels + pred_label\n",
    "    labels[labels > 1.1] = 1\n",
    "\n",
    "    final_labels = true_labels\n",
    "    bin_label_out = labels\n",
    "\n",
    "\n",
    "    norm_class = \"SNR\"\n",
    "    equiv_index = [\"RBBB\", \"SVPB\", \"VPB\"]\n",
    "\n",
    "    final_classes = [elem for elem in weightindex if elem not in equiv_index]\n",
    "\n",
    "    ind = np.any(weights, axis=0)  # Find indices of classes in weight matrix.\n",
    "    final_classes = [j for i, j in enumerate(final_classes) if ind[i]]\n",
    "    final_labels = final_labels[:, ind]\n",
    "    binary_outputs = bin_label_out[:, ind]\n",
    "    weights = weights[np.ix_(ind, ind)]\n",
    "\n",
    "    challenge_metric_val = challenge_metric(weights, labels, binary_outputs, final_classes, norm_class)\n",
    "\n",
    "    return challenge_metric_val\n",
    "\n",
    "\n",
    "# Compute modified confusion matrix for multi-class, multi-label tasks.\n",
    "def compute_modified_confusion_matrix(labels, outputs):\n",
    "    # Compute a binary multi-class, multi-label confusion matrix, where the rows\n",
    "    # are the labels and the columns are the outputs.\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    A = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    # Iterate over all of the recordings.\n",
    "    for i in range(num_recordings):\n",
    "        # Calculate the number of positive labels and/or outputs.\n",
    "        normalization = float(max(np.sum(np.any((labels[i, :], outputs[i, :]), axis=0)), 1))\n",
    "        # Iterate over all of the classes.\n",
    "        for j in range(num_classes):\n",
    "            # Assign full and/or partial credit for each positive class.\n",
    "            if labels[i, j]:\n",
    "                for k in range(num_classes):\n",
    "                    if outputs[i, k]:\n",
    "                        A[j, k] += 1.0 / normalization\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def challenge_metric(weights, labels, outputs, classes, normal_class):\n",
    "    num_recordings, num_classes = np.shape(labels)\n",
    "    normal_index = classes.index(normal_class)\n",
    "\n",
    "    # Compute the observed score.\n",
    "    A = compute_modified_confusion_matrix(labels, outputs)\n",
    "    observed_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the correct label(s).\n",
    "    correct_outputs = labels\n",
    "    A = compute_modified_confusion_matrix(labels, correct_outputs)\n",
    "    correct_score = np.nansum(weights * A)\n",
    "\n",
    "    # Compute the score for the model that always chooses the normal class.\n",
    "    inactive_outputs = np.zeros((num_recordings, num_classes), dtype=np.bool)\n",
    "    inactive_outputs[:, normal_index] = 1\n",
    "    A = compute_modified_confusion_matrix(labels, inactive_outputs)\n",
    "    inactive_score = np.nansum(weights * A)\n",
    "\n",
    "    if correct_score != inactive_score:\n",
    "        normalized_score = float(observed_score - inactive_score) / float(correct_score - inactive_score)\n",
    "    else:\n",
    "        normalized_score = float('nan')\n",
    "\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12, 5000])\n",
      "----------------------------------------------------------------------------\n",
      "           Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "               Conv1d-1         [64, 64, 8]       3,840,000       3,840,000\n",
      "          BatchNorm1d-2         [64, 64, 8]             128             128\n",
      "                 ReLU-3         [64, 64, 8]               0               0\n",
      "            MaxPool1d-4         [64, 64, 4]               0               0\n",
      "           BasicBlock-5         [64, 64, 4]          57,600          57,600\n",
      "           BasicBlock-6         [64, 64, 4]          57,600          57,600\n",
      "           BasicBlock-7         [64, 64, 4]          57,600          57,600\n",
      "           BasicBlock-8        [64, 128, 2]         180,992         180,992\n",
      "           BasicBlock-9        [64, 128, 2]         229,888         229,888\n",
      "          BasicBlock-10        [64, 128, 2]         229,888         229,888\n",
      "          BasicBlock-11        [64, 128, 2]         229,888         229,888\n",
      "          BasicBlock-12        [64, 256, 1]         722,432         722,432\n",
      "          BasicBlock-13        [64, 256, 1]         918,528         918,528\n",
      "          BasicBlock-14        [64, 256, 1]         918,528         918,528\n",
      "          BasicBlock-15        [64, 256, 1]         918,528         918,528\n",
      "          BasicBlock-16        [64, 256, 1]         918,528         918,528\n",
      "          BasicBlock-17        [64, 256, 1]         918,528         918,528\n",
      "          BasicBlock-18        [64, 512, 1]       2,886,656       2,886,656\n",
      "          BasicBlock-19        [64, 512, 1]       3,672,064       3,672,064\n",
      "          BasicBlock-20        [64, 512, 1]       3,672,064       3,672,064\n",
      "   AdaptiveAvgPool1d-21        [64, 512, 1]               0               0\n",
      "              Linear-22           [64, 111]          56,943          56,943\n",
      "============================================================================\n",
      "Total params: 20,486,383\n",
      "Trainable params: 20,486,383\n",
      "Non-trainable params: 0\n",
      "Batch size: 64\n",
      "----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'----------------------------------------------------------------------------\\n           Layer (type)        Output Shape         Param #     Tr. Param #\\n============================================================================\\n               Conv1d-1         [64, 64, 8]       3,840,000       3,840,000\\n          BatchNorm1d-2         [64, 64, 8]             128             128\\n                 ReLU-3         [64, 64, 8]               0               0\\n            MaxPool1d-4         [64, 64, 4]               0               0\\n           BasicBlock-5         [64, 64, 4]          57,600          57,600\\n           BasicBlock-6         [64, 64, 4]          57,600          57,600\\n           BasicBlock-7         [64, 64, 4]          57,600          57,600\\n           BasicBlock-8        [64, 128, 2]         180,992         180,992\\n           BasicBlock-9        [64, 128, 2]         229,888         229,888\\n          BasicBlock-10        [64, 128, 2]         229,888         229,888\\n          BasicBlock-11        [64, 128, 2]         229,888         229,888\\n          BasicBlock-12        [64, 256, 1]         722,432         722,432\\n          BasicBlock-13        [64, 256, 1]         918,528         918,528\\n          BasicBlock-14        [64, 256, 1]         918,528         918,528\\n          BasicBlock-15        [64, 256, 1]         918,528         918,528\\n          BasicBlock-16        [64, 256, 1]         918,528         918,528\\n          BasicBlock-17        [64, 256, 1]         918,528         918,528\\n          BasicBlock-18        [64, 512, 1]       2,886,656       2,886,656\\n          BasicBlock-19        [64, 512, 1]       3,672,064       3,672,064\\n          BasicBlock-20        [64, 512, 1]       3,672,064       3,672,064\\n   AdaptiveAvgPool1d-21        [64, 512, 1]               0               0\\n              Linear-22           [64, 111]          56,943          56,943\\n============================================================================\\nTotal params: 20,486,383\\nTrainable params: 20,486,383\\nNon-trainable params: 0\\nBatch size: 64\\n----------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = resnet34().double()\n",
    "\n",
    "x = torch.from_numpy(trainingleads[0:64,:,:])\n",
    "print(x.shape)\n",
    "y = model(x)\n",
    "pms.summary(model, x, max_depth=None, show_input=False, batch_size=64, print_summary=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41856866a918aca834febfb2afb95d5156a9df57ae4766dfcf712b5e394b8ee1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('python38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
